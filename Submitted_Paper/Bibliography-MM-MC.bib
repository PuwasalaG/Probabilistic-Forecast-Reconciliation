@inproceedings{GasEtAl2019,
  title={Probabilistic forecasting with spline quantile function RNNs},
  author={Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1901--1910},
  year={2019},
  organization={PMLR}
}

@misc{HofEtAl2020,
      title={Applications of multivariate quasi-random sampling with neural networks},
      author={Marius Hofert and Avinash Prasad and Mu Zhu},
      year={2020},
      eprint={2012.08036},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{JanSte2020,
   title={Probabilistic multivariate electricity price forecasting using implicit generative ensemble post-processing},
   ISBN={9781728128221},
   url={http://dx.doi.org/10.1109/PMAPS47429.2020.9183687},
   DOI={10.1109/pmaps47429.2020.9183687},
   journal={2020 International Conference on Probabilistic Methods Applied to Power Systems (PMAPS)},
   publisher={IEEE},
   author={Janke, Tim and Steinke, Florian},
   year={2020},
   month={Aug}
}
@Manual{bigvar,
    title = {BigVAR: Dimension Reduction Methods for Multivariate Time Series},
    author = {Will Nicholson and David Matteson and Jacob Bien},
    year = {2019},
    note = {R package version 1.0.6},
    url = {https://CRAN.R-project.org/package=BigVAR},
  }
@article{AthEtAl2017,
  title={Forecasting with temporal hierarchies},
  author={Athanasopoulos, George and Hyndman, Rob J and Kourentzes, Nikolaos and Petropoulos, Fotios},
  journal={European Journal of Operational Research},
  volume={262},
  number={1},
  pages={60--74},
  year={2017},
  publisher={Elsevier}
}
@book{FPP2018,
  title={Forecasting: Principles and Practice},
  edition = {3rd},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2021},
  publisher={OTexts},
  address = {Melbourne, Australia}
}
@article{JeoEtAl2019,
abstract = {New methods are proposed for adjusting probabilistic forecasts to ensure coherence with the aggregation constraints inherent in temporal hierarchies. The different approaches nested within this framework include methods that exploit information at all levels of the hierarchy as well as a novel method based on cross-validation. The methods are evaluated using real data from two wind farms in Crete and electric load in Boston. For these applications, optimal decisions related to grid operations and bidding strategies are based on coherent probabilistic forecasts of energy power. Empirical evidence is also presented showing that probabilistic forecast reconciliation improves the accuracy of the probabilistic forecasts.},
author = {Jeon, Jooyoung and Panagiotelis, Anastasios and Petropoulos, Fotios},
doi = {10.1016/j.ejor.2019.05.020},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Aggregation,Cross-validation,Forecasting,Renewable energy generation,Temporal hierarchies},
number = {2},
pages = {364--379},
publisher = {Elsevier B.V.},
title = {{Probabilistic forecast reconciliation with applications to wind power and electric load}},
volume = {279},
year = {2019}
}
@article{Yao2006,
abstract = {Abstract.  We provide a direct proof for consistency and asymptotic normality of Gaussian maximum likelihood estimators for causal and invertible autoregressive moving-average (ARMA) time series models, which were initially established by Hannan [Journal of Applied Probability (1973) vol. 10, pp. 130–145] via the asymptotic properties of a Whittle's estimator. This also paves the way to establish similar results for spatial processes presented in the follow-up article by Yao and Brockwell [Bernoulli (2006) in press].},
author = {Yao, Qiwei and Brockwell, Peter J.},
doi = {10.1111/j.1467-9892.2006.00492.x},
journal = {Journal of Time Series Analysis},
keywords = {ARMA time series models,Asymptotic normality,Consistency,Gaussian maximum likelihood estimator,Innovation algorithm,Martingale difference,Prewhitening},
number = {6},
pages = {857--875},
title = {{Gaussian maximum likelihood estimation for ARMA models. I. Time series}},
volume = {27},
year = {2006}
}
@article{Shang2017,
abstract = {Age-specific mortality rates are often disaggregated by different attributes, such as sex, state and ethnicity. Forecasting age-specific mortality rates at the national and sub-national levels plays an important role in developing social policy. However, independent forecasts at the sub-national levels may not add up to the forecasts at the national level. To address this issue, we consider reconciling forecasts of age-specific mortality rates, extending the methods of Hyndman et al. (2011) to functional time series, where age is considered as a continuum. The grouped functional time series methods are used to produce point forecasts of mortality rates that are aggregated appropriately across different disaggregation factors. For evaluating forecast uncertainty, we propose a bootstrap method for reconciling interval forecasts. Using the regional age-specific mortality rates in Japan, obtained from the Japanese Mortality Database, we investigate the one- to ten-step-ahead point and interval forecast accuracies between the independent and grouped functional time series forecasting methods. The proposed methods are shown to be useful for reconciling forecasts of age-specific mortality rates at the national and sub-national levels. They also enjoy improved forecast accuracy averaged over different disaggregation factors. Supplemental materials for the article are available online.},
archivePrefix = {arXiv},
arxivId = {1609.04222},
author = {Shang, Han Lin and Hyndman, Rob J.},
doi = {10.1080/10618600.2016.1237877},
eprint = {1609.04222},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Bottom-up,Forecast reconciliation,Hierarchical time series forecasting,Japanese mortality database,Optimal combination},
number = {2},
pages = {330--343},
title = {{Grouped Functional Time Series Forecasting: An Application to Age-Specific Mortality Rates}},
volume = {26},
year = {2017}
}
@Manual{Rforecast,
title = {{forecast}: Forecasting Functions for Time Series and Linear Models},
year = {2020},
author = {Rob J Hyndman and George Athanasopoulos and Christoph Bergmeir and Gabriel Caceres and Leanne Chhay and Mitchell O'Hara-Wild and Fotios Petropoulos and Slava Razbash and Earo Wang and Farah Yasmeen and {R Core Team} and Ross Ihaka and Daniel Reid and David Shaub and Yuan Tang and Zhenyu Zhou},
url = {https://CRAN.R-project.org/package=forecast},
note = {Version 8.12},
}
@article{WicEtAl2019,
abstract = {AbstractLarge collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series, a constraint we refer to as “coherence”. Forecast reconciliation is the process of adjusting forecasts to make them coherent. The reconciliation algorithm proposed by Hyndman et al. (2011) is based on a generalized least squares estimator that requires an estimate of the covariance matrix of the coherency errors (i.e., the errors that arise due to incoherence). We show that this matrix is impossible to estimate in practice due to identifiability conditions. We propose a new forecast reconciliation approach that incorporates the information from a full covariance matrix of forecast errors in obtaining a set of coherent forecasts. Our approach minimizes the mean squared error of the coherent forecasts across the entire collection of time series under the assumption of unbiasedness. The minimization problem has a closed form solution. We make this solution scalable by providing a computationally efficient representation. We evaluate the performance of the proposed method compared to alternative methods using a series of simulation designs which take into account various features of the collected time series. This is followed by an empirical application using Australian domestic tourism data. The results indicate that the proposed method works well with artificial and real data.},
author = {Wickramasuriya, Shanika L. and Athanasopoulos, George and Hyndman, Rob J.},
doi = {10.1080/01621459.2018.1448825},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Aggregation,Australian tourism,Coherent forecasts,Contemporaneous error correlation,Forecast combinations,Spatial correlations},
number = {526},
pages = {804--819},
publisher = {Taylor {\&} Francis},
title = {Optimal Forecast Reconciliation for Hierarchical and Grouped Time Series Through Trace Minimization},
volume = {114},
year = {2019}
}
@article{Hyndman2016,
abstract = {It is shown that the least squares approach to reconciling hierarchical time series forecasts can be extended to much more general collections of time series with aggregation constraints. The constraints arise due to the need for forecasts of collections of time series to add up in the same way as the observed time series. It is also shown that the computations involved can be handled efficiently by exploiting the structure of the associated design matrix, or by using sparse matrix routines. The proposed algorithms make forecast reconciliation feasible in business applications involving very large numbers of time series.},
author = {Hyndman, Rob J. and Lee, Alan J. and Wang, Earo},
doi = {10.1016/j.csda.2015.11.007},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Combining forecasts,Grouped time series,Hierarchical time series,Reconciling forecasts,Weighted least squares},
pages = {16--32},
publisher = {Elsevier B.V.},
title = {Fast computation of reconciled forecasts for hierarchical and grouped time series},
volume = {97},
year = {2016}
}
@article{Schafer2005,
abstract = {{\textless}p{\textgreater}Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.{\textless}/p{\textgreater}},
author = {Sch{\"{a}}fer, Juliane and Strimmer, Korbinian},
doi = {10.2202/1544-6115.1175},
journal = {Statistical Applications in Genetics and Molecular Biology},
number = {1},
pmid = {16646851},
title = {A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics},
volume = {4},
year = {2005}
}
@article{Hall2007,
abstract = {This paper brings together two important but hitherto largely unrelated areas of the forecasting literature, density forecasting and forecast combination. It proposes a practical data-driven approach to the direct combination of density forecasts by taking a weighted linear combination of the competing density forecasts. The combination weights are chosen to minimize the 'distance', as measured by the Kullback-Leibler information criterion, between the forecasted and true but unknown density. We explain how this minimization both can and should be achieved but leave theoretical analysis to future research. Comparisons with the optimal combination of point forecasts are made. An application to simple time-series density forecasts and two widely used published density forecasts for U.K. inflation, namely the Bank of England and NIESR "fan" charts, illustrates that combination can but need not always help. {\textcopyright} 2006 International Institute of Forecasters.},
author = {Hall, Stephen G. and Mitchell, James},
doi = {10.1016/j.ijforecast.2006.08.001},
journal = {International Journal of Forecasting},
keywords = {Combining forecasts,Density forecasts,Evaluating forecasts,Inflation forecasting,Uncertainty},
number = {1},
pages = {1--13},
title = {{Combining density forecasts}},
volume = {23},
year = {2007}
}
@article{Jordan2017,
abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
archivePrefix = {arXiv},
arxivId = {1709.04743},
author = {Jordan, Alexander and Kr{\"{u}}ger, Fabian and Lerch, Sebastian},
eprint = {1709.04743},
keywords = {1,and being able to,by uncertainty,comparative evaluation,comparative evaluation, ensemble forecasts, out-of,distributions,ensemble forecasts,forecast evaluation,forecasts are generally surrounded,introduction,out-of-sample evaluation,predictive,proper scoring rules,quantify this uncertainty,r,score calculation},
title = {{Evaluating probabilistic forecasts with the R package scoringRules}},
url = {http://arxiv.org/abs/1709.04743},
year = {2017}
}
@article{SCHEUERER2015,
author = {Scheuerer, Michael and Hamill, Thomas M.},
doi = {10.1175/MWR-D-14-00269.1},
journal = {Monthly Weather Review},
number = {4},
pages = {1321--1334},
title = {Variogram-Based Proper Scoring Rules for Probabilistic Forecasts of Multivariate Quantities},
volume = {143},
year = {2015}
}
@article{Szekely2013,
abstract = {Energy distance is a statistical distance between the distributions of random vectors, which characterizes equality of distributions. The name energy derives from Newton's gravitational potential energy, and there is an elegant relation to the notion of potential energy between statistical observations. Energy statistics are functions of distances between statistical observations in metric spaces. Thus even if the observations are complex objects, like functions, one can use their real valued nonnegative distances for inference. Theory and application of energy statistics are discussed and illustrated. Finally, we explore the notion of potential and kinetic energy of goodness-of-fit. {\textcopyright} 2013 Elsevier B.V.},
author = {Sz{\'{e}}kely, G{\'{a}}bor J. and Rizzo, Maria L.},
doi = {10.1016/j.jspi.2013.03.018},
journal = {Journal of Statistical Planning and Inference},
keywords = {Distance correlation,Distance covariance,Energy distance,Goodness-of-fit,Multivariate independence},
number = {8},
pages = {1249--1272},
publisher = {Elsevier},
title = {{Energy statistics: A class of statistics based on distances}},
volume = {143},
year = {2013}
}
@techreport{Pinson2013a,
abstract = {Research on generating and verification of multivariate probabilistic forecasts has gained increased interest over the last few years. Emphasis is placed here on the evaluation of forecast quality with the Energy score, which is based on a quadratic scoring rule. While this score may be seen as appealing since being proper, we show that its discrimination ability may be limited when focusing on the dependence structure of multivariate probabilistic forecasts. For the case of multivariate Gaussian process, a theoretical upper for such discrimination ability is derived and discussed. This limited discrimination ability may eventually get compromised by computational and sampling issues, as dimension increases.},
author = {Pinson, P and Tastu, Julija},
institution = {Technical University of Denmark.},
keywords = {Discrimination,Energy score,Multivariate scenarios BT - Discrimination ability,Probabilistic forecasting,Proper score},
title = {Discrimination ability of the Energy score},
year = {2013}
}
@article{Gneiting2007,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G?=F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster tomake careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed.We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
number = {477},
pages = {359--378},
title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
volume = {102},
year = {2007}
}
@article{BenTaieb2017,
abstract = {Smart electricity meters are currently deployed in millions of households to collect detailed individual electricity consumption data. Compared to traditional electricity data based on aggregated consumption, smart meter data are much more volatile and less predictable. There is a need within the energy industry for probabilistic forecasts of household electricity consumption to quantify the uncertainty of future electricity demand, in order to undertake appropriate planning of generation and distribution. We propose to estimate an additive quantile regression model for a set of quantiles of the future distribution using a boosting procedure. By doing so, we can benefit from flexible and interpretable models which include an automatic variable selection. We compare our approach with three benchmark methods on both aggregated and disaggregated scales using a smart meter dataset collected from 3639 households in Ireland at 30-minute intervals over a period of 1.5 years. The empirical results demonstrate that our approach based on quantile regression provides better forecast accuracy for disaggregated demand while the traditional approach based on a normality assumption (possibly after an appropriate Box- Cox transformation) is a better approximation for aggregated demand. These results are particularly useful since more energy data will become available at the disaggregated level in the future. I. INTRODUCTION T HE energy sector has been changing dramatically, notably due to the integration of renewable energy sources, in an effort to reduce our dependency on fossil fuels and achieve a better sustainable future. With the growing amount of data from energy systems, there is a need for utilities to quantify the uncertainty in future generation and demand, especially for wind power [1], solar power [2] and electricity demand [3]. In particular, accurate probabilistic forecasts for electricity demand are critical for electric utilities in many operational and planning tasks. Electricity load is often represented as the aggregated load across many households (e.g. at the city level). There is also a rich literature on forecasting the average aggregated electricity load; i.e. in forecasting the mean of the future demand distribution. These forecasts are often conditional on a number of predictor variables such as calendar and temperature variables. Many models have been considered S. BenTaieb is with the King Abdullah University of Science and Tech- nology (KAUST), Thuwal, Saudi Arabia, and the Monash Business School, Clayton, VIC 3800, Australia (e-mails: souhaib.bentaieb@monash.edu). R. Huser is with the King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia (e-mail: raphael.huser@kaust.edu.sa). R. J. Hyndman is with the Monash Business School, Clayton, VIC 3800, Australia (e-mail: rob.hyndman@monash.edu). M. G. Genton is with the King Abdullah University of Science and Tech- nology (KAUST), Thuwal, Saudi Arabia (e-mail: marc.genton@kaust.edu.sa). for},
author = {{Ben Taieb}, Souhaib and Huser, Raphael and Hyndman, Rob J. and Genton, Marc G.},
doi = {10.1109/TSG.2016.2527820},
issn = {19493053},
journal = {IEEE Transactions on Smart Grid},
keywords = {Probabilistic load forecasting,gradient boosting,quantile regression,smart meters},
number = {5},
pages = {2448--2455},
title = {{Forecasting uncertainty in electricity smart meter data by boosting additive quantile regression}},
volume = {7},
year = {2017}
}
@article{McSharry2005,
abstract = {Adequate capacity planning requires accurate forecasts of the future magnitude and timing of peak electricity demand. Electricity demand is affected by the day of the week, seasonal variations, holiday periods, feast days, and the weather. A model that provides probabilistic forecasts of both magnitude and timing for lead times of one year is presented. This model is capable of capturing the main sources of variation in demand and uses simulated weather time series, including temperature, wind speed, and luminosity, for producing probabilistic forecasts of future peak demand. Having access to such probabilistic forecasts provides a means of assessing the uncertainty in the forecasts and can lead to improved decision making and better risk management. {\&}copy; 2005 IEEE.},
author = {McSharry, Patrick E. and Bouwman, Sonja and Bloemhof, Gabri{\"{e}}l},
doi = {10.1109/TPWRS.2005.846071},
journal = {IEEE Transactions on Power Systems},
keywords = {Load forecasting,Load management,Management decision making,Power demand,Power generation peaking capacity,Power system planning,Simulation,Temperature,Time series},
number = {2},
pages = {1166--1172},
title = {{Probabilistic forecasts of the magnitude and timing of peak electricity demand}},
volume = {20},
year = {2005}
}
@article{Gneiting2005b,
author = {Gneiting, Tilmann and Raftery, Andrian E.},
journal = {Science},
pages = {248--249},
title = {Weather Forecasting with Ensemble Methods},
volume = {310.5746},
year = {2005}
}
@article{Pinson2009,
abstract = {Short-term (up to 2-3 days ahead) probabilistic forecasts of wind power provide forecast users with a highly valuable information on the uncertainty of expected wind generation. Whatever the type of these probabilistic forecasts, they are produced on a per horizon basis, and hence do not inform on the devel- opment of the forecast uncertainty through forecast series. However, this additional informationmay be paramount for a large class of time-dependent and multi-stage decision-making problems e.g. optimal operation of combined wind-storage systems or multiple-market trading with different gate closures. This issue is addressed here by describing amethod that permits the generation of statistical scenarios of short-termwind generation that accounts for both the interdependence structure of prediction errors and the predictive distributions of wind power production. The method is based on the conversion of series of prediction errors to a multivariate Gaussian random variable, the interdependence structure of which can then be summarized by a unique covariance matrix. Such matrix is recursively estimated in order to accommodate long-term variations in the prediction error characteristics. The quality and interest of the methodology are demonstrated with an application to the test case of a multi-MWwind farm over a period of more than two years. Keywords:},
author = {Pinson, Pierre and Madsen, Henrik and Papaefthymiou, George and Kl{\"{o}}ckl, Bernd},
doi = {10.1002/we.284},
issn = {10954244},
journal = {Wind Energy},
keywords = {forecasting,multivariate Gaussian random variable,scenarios,uncertainty,wind power},
number = {1},
pages = {51--62},
title = {From Probabilistic Forecasts to Wind Power Production},
volume = {12},
year = {2009}
}
@article{Gneiting2005a,
abstract = {Ensemble prediction systems typically show positive spread-error correlation, but they are subject to forecast bias and dispersion errors, and are therefore uncalibrated. This work proposes the use of ensemble model output statistics (EMOS), an easy-to-implement postprocessing technique that addresses both forecast bias and underdispersion and takes into account the spread-skill relationship. The technique is based on multiple linear regression and is akin to the superensemble approach that has traditionally been used for deterministic-style forecasts. The EMOS technique yields probabilistic forecasts that take the form of Gaussian predictive probability density functions (PDFs) for continuous weather variables and can be applied to gridded model output. The EMOS predictive mean is a bias-corrected weighted average of the ensemble member forecasts, with coefficients that can be interpreted in terms of the relative contributions of the member models to the ensemble, and provides a highly competitive deterministic-style forecast. The EMOS predictive variance is a linear function of the ensemble variance. For fitting the EMOS coefficients, the method of minimum continuous ranked probability score (CRPS) estimation is introduced. This technique finds the coefficient values that optimize the CRPS for the training data. The EMOS technique was applied to 48-h forecasts of sea level pressure and surface temperature over the North American Pacific Northwest in spring 2000, using the University of Washington mesoscale ensemble. When compared to the bias-corrected ensemble, deterministic-style EMOS forecasts of sea level pressure had root-mean-square error 9{\%} less and mean absolute error 7{\%} less. The EMOS predictive PDFs were sharp, and much better calibrated than the raw ensemble or the bias-corrected ensemble.},
author = {Gneiting, Tilmann and Raftery, Adrian E. and Westveld, Anton H. and Goldman, Tom},
doi = {10.1175/MWR2904.1},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {5},
pages = {1098--1118},
title = {Calibrated Probabilistic Forecasting Using Ensemble Model Output Statistics and Minimum {CRPS} Estimation},
volume = {133},
year = {2005}
}
@article{Gneiting2006,
abstract = {With the global proliferation of wind power, the need for accurate short-term forecasts of wind resources at wind energy sites is becoming paramount. Regime-switching space-time (RST) models merge meteorological and statistical expertise to obtain accurate and calibrated, fully probabilistic forecasts of wind speed and wind power. The model formulation is parsimonious, yet takes into account all of the salient features of wind speed: alternating atmospheric regimes, temporal and spatial correlation, diurnal and seasonal nonstationarity, conditional heteroscedasticity, and non-Gaussianity. The RST method identifies forecast regimes at a wind energy site and fits a conditional predictive model for each regime. Geographically dispersed meteorological observations in the vicinity of the wind farm are used as off-site predictors. The RST technique was applied to 2-hour-ahead forecasts of hourly average wind speed near the Stateline wind energy center in the U.S. Pacific Northwest. The RST point forecasts and distributional forecasts were accurate, calibrated, and sharp, and they compared favorably with predictions based on state-of-the-art time series techniques. This suggests that quality meteorological data from sites upwind of wind farms can be efficiently used to improve short-term forecasts of wind resources.},
author = {Gneiting, Tilmann and Larson, Kristin and Westrick, Kenneth and Genton, Marc G and Aldrich, Eric},
doi = {10.1198/016214506000000456},
journal = {Journal of the American Statistical Association},
keywords = {continuous ranked probability score,minimum continuous ranked probability,predictive distribution,score estimation,spatiotemporal,truncated normal,weather prediction},
number = {475},
pages = {968--979},
title = {{Calibrated Probabilistic Forecasting at the Stateline Wind Energy Center}},
volume = {101},
year = {2006}
}
@article{Gel2004,
abstract = {Probabilistic weather forecasting consists of finding a joint probability distribution for future weather quantities or events. It is typically done by using a numerical weather prediction model, perturbing the inputs to the model in various ways, and running the model for each perturbed set of inputs. The result is then viewed as an ensemble of forecasts, taken to be a sample from the joint probability distribution of the future weather quantities of interest. This is typically not feasible for mesoscale weather prediction carried out locally by organizations without the vast data and computing resources of national weather centers. Instead, we propose a simpler method that breaks with much previous practice by perturbing the outputs, or deterministic forecasts, from the model. Forecast errors are modeled using a geostatistical model, and ensemble members are generated by simulating realizations of the geostatistical model. The method is applied to 48-hour mesoscale forecasts of temperature in the North ...},
author = {Gel, Yulia and Raftery, Adrian E and Gneiting, Tilmann},
doi = {10.1198/016214504000000872},
journal = {Journal of the American Statistical Association},
keywords = {empirical calibration,ensemble forecast,geostatistical simulation,probabilistic weather prediction},
number = {July},
pages = {575--583},
title = {{Calibrated Probabilistic Mesoscale Weather Field Forecasting}},
volume = {99},
year = {2004}
}
@article{Pinson2012,
author = {Pinson, Pierre},
doi = {10.1002/qj.1873},
journal = {Quarterly Journal of the Royal Meteorological Society},
keywords = {bivariate processes,ensemble prediction,near-,probabilistic calibration,recursive estimation},
pages = {1273--1284},
title = {Adaptive calibration of $(u, v)$-wind ensemble forecasts},
volume = {138},
year = {2012}
}
@article{Tay2000,
abstract = {A density forecast of the realization of a random variable at some future time is an estimate of the probability distribution of the possible future values of that variable. This article presents a selective survey of applications of density forecasting in macroeconomics and finance, and discusses some issues concerning the production, presentation, and evaluation of density forecasts. Copyright (C) 2000 John Wiley {\&} Sons, Ltd.},
author = {Tay, Anthony S. and Wallis, Kenneth F.},
doi = {10.1002/9780470996430.ch3},
journal = {Journal of Forecasting},
keywords = {{\_}nancial forecasts,density forecasts,economic forecasts,forecast evaluation,probability distributions},
pages = {124--143},
title = {{Density forecasting: A survey}},
volume = {19},
year = {2000}
}
@article{Abramson1995,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Abramson, Bruce and Clemen, Robert},
doi = {10.1016/0169-2070(94)02000-F},
eprint = {arXiv:1011.1669v3},
journal = {International Journal of Forecasting},
number = {1},
pages = {1--4},
pmid = {25246403},
title = {{Probability forecasting}},
volume = {11},
year = {1995}
}
@misc{Kahn1998,
abstract = {Encourages a hybrid approach consisting of top-down and bottom-up sales forecasting. Comparison of the two types of forecasting; Forecasting issues companies face; Data pattern characteristics; Comparisons of mean absolute error across forecast levels.},
author = {Kahn, Kenneth B},
booktitle = {Journal of Business Forecasting Methods {\&} Systems},
keywords = {SALES forecasting},
number = {2},
pages = {14},
title = {{Revisiting top-down versus bottom-up forecasting}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=bth{\&}AN=985713{\&}lang=pt-br{\&}site=ehost-live},
volume = {17},
year = {1998}
}
@article{Schwarzkopf1988,
author = {Schwarzkopf, Albert B.1 and Tersine, Richard J.1 and Morris, John S.2},
doi = {10.1080/00207548808947995},
issn = {0020-7543},
journal = {International Journal of Production Research},
number = {11},
pages = {1833},
title = {{Top-down versus bottom-up forecasting strategies.}},
volume = {26},
year = {1988}
}
@article{Gneiting2014,
abstract = {Aprobabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibra- tion, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilis- tic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharp- ness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts.We em- phasizemethodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed en- semble forecasts in numerical weather prediction as an example. Through- out, we illustrate concepts and methodologies in data examples. 125},
author = {Gneiting, Tilmann and Katzfuss, Matthias},
doi = {10.1146/annurev-statistics-062713-085831},
journal = {Annual Review of Statistics and Its Application},
keywords = {calibration,consistent scoring function,distributional regression,ensemble forecast,proper scoring,rule},
pages = {125--151},
title = {Probabilistic Forecasting},
volume = {1},
year = {2014}
}
@article{Gneiting2008,
abstract = {We discuss methods for the evaluation of probabilistic predictions of vector-valued quantities, that can take the form of a discrete forecast ensemble or a density forecast. In particular, we propose a multivariate version of the univariate verification rank histogram or Talagrand diagram that can be used to check the calibration of ensemble forecasts. In the case of density forecasts, Box's density ordinate transform provides an attractive alternative. The multivariate energy score generalizes the continuous ranked probability score. It addresses both calibration and sharpness, and can be used to compare deterministic forecasts, ensemble forecasts and density forecasts, using a single loss function that is proper. An application to the University of Washington mesoscale ensemble points at strengths and deficiencies of probabilistic short-range forecasts of surface wind vectors over the North American Pacific Northwest.},
author = {Gneiting, Tilmann and Stanberry, Larissa I. and Grimit, Eric P. and Held, Leonhard and Johnson, Nicholas A.},
doi = {10.1007/s11749-008-0114-x},
isbn = {1174900801186},
issn = {11330686},
journal = {Test},
keywords = {Calibration,Density forecast,Ensemble postprocessing,Exchangeability,Forecast verification,Probability integral transform,Proper scoring rule,Rank histogram,Sharpness},
number = {2},
pages = {211--235},
title = {Assessing probabilistic forecasts of multivariate quantities, with an application to ensemble predictions of surface winds},
volume = {17},
year = {2008}
}
@article{Lapide1998,
author = {Lapide, Larry},
journal = {Journal of Business Forecasting Methods {\&} Systems},
pages = {28--31},
title = {{A simple view of top-down vs bottom-up forecasting}},
volume = {17},
year = {1998}
}
@article{Fliedner2001,
abstract = {In order to provide the appropriate demand forecast information given various managerial levels and functional disciplines within organizations, reliance on family-based forecasting is increasing. The family-based approach, sometimes referred to as hierarchical forecasting (HF), is based on a strategy of aggregating items into families. HF systems are capable of providing forecasts for items and their respective families. The objectives of HF systems, include improved forecast performance and a reduction in the overall forecasting burden. To date, several studies have offered practical guidelines for the structural design of HF systems. The primary purpose of this paper is to summarize these guidelines. First, an explanation of the HF process is provided. In this explanation, important system parameters and strategic choices, which allow for the custom configuration of HF systems are identified. Second, the relevant family-based forecast research is reviewed. The important issues addressed and the conclusions presented in this research are identified. Third, practical guidelines regarding the use of a HF approach that have been reported in the research literature are clearly delineated. With much still unknown regarding the performance impact of various system parameter and strategic process choices, the paper concludes with suggestions for future research.},
author = {Fliedner, Gene},
doi = {10.1108/02635570110365952},
journal = {Industrial Management {\&} Data Systems},
keywords = {forecasting,hierarchy,product management,time series},
number = {1},
pages = {5--12},
title = {{Hierarchical forecasting: issues and use guidelines}},
volume = {101},
year = {2001}
}
@article{Dunn1976,
abstract = {Should statistical forecasts be constructed by aggregating data to each level for which forecasts are required or aggregating the forecasts from the lower levels? The relevant literature suggests no general answer. In this study using actual data, forecasts aggregated from lower-level modeling were found best.},
author = {Dunn, D M and Williams, W H and Dechaine, T L},
doi = {10.2307/2285732},
journal = {Journal of the American Statistical Association},
number = {353},
pages = {68--71},
title = {Aggregate Versus Subaggregate Models in Local Area Forecasting},
volume = {71},
year = {1976}
}
@Article{HynEtAl2011,
author = {Hyndman, Rob J. and Ahmed, Roman A. and Athanasopoulos, George and Shang, Han Lin},
title = {Optimal combination forecasts for hierarchical time series},
journal = {Computational Statistics and Data Analysis},
year = {2011},
volume = {55},
number = {9},
pages = {2579--2589},
doi = {10.1016/j.csda.2011.03.006},
isbn = {0167-9473},
keywords = {Bottom-up forecasting,Combining forecasts,GLS regression,Hierarchical forecasting,Reconciling forecasts,Top-down forecasting},
publisher = {Elsevier B.V.},
}
@article{Gross1990,
abstract = {This paper addresses the issue of forecasting individual items within a product line; where each line includes several independent but closely related products. The purpose of the research was to reduce the overall forecasting burden by developing and assessing schemes of disaggregating forecasts of a total product line to the related individual items. Measures were developed to determine appropriate disaggregated methodologies and to compare the forecast accuracy of individual product forecasts versus disaggregated totals. Several of the procedures used were based upon extensions of the combination of forecast research and applied to disaggregations of total forecasts of product lines. The objective was to identify situations when it was advantageous to produce disaggregated forecasts, and if advantageous, which method of disaggregation to utilize. This involved identification of the general conceptual characteristics within a set of product line data that might cause a disaggregation method to produce relatively accurate forecasts. These conceptual characteristics provided guidelines for forecasters on how to select a disaggregation method and under what conditions a particular method is applicable.},
author = {Gross, Charles W. and Sohl, Jeffrey E.},
doi = {10.1002/for.3980090304},
journal = {Journal of Forecasting},
keywords = {Composite root mean square error differential,Disaggregational methods,Forecasting,Product line,Time series analysis},
number = {3},
pages = {233--254},
title = {{Disaggregation methods to expedite product line forecasting}},
volume = {9},
year = {1990}
}
@incollection{VanErven2015a,
title={Game-theoretically optimal reconciliation of contemporaneous hierarchical time series forecasts},
author={Van Erven, Tim and Cugliari, Jairo},
booktitle={Modeling and Stochastic Learning for Forecasting in High Dimensions},
pages={297--317},
year={2015},
publisher={Springer}
}

@Manual{Rcore,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2018},
    url = {https://www.R-project.org/},
}

@article{AthEtAl2009,
title = "Hierarchical forecasts for {Australian} domestic tourism",
journal = "International Journal of Forecasting",
volume = "25",
number = "1",
pages = "146 - 166",
year = "2009",
issn = "0169-2070",
doi = "https://doi.org/10.1016/j.ijforecast.2008.07.004",
author = "George Athanasopoulos and Roman A. Ahmed and Rob J. Hyndman",
keywords = "Australia, Exponential smoothing, Hierarchical forecasting, Innovations state space models, Optimal combination forecasts, Top-down method, Tourism demand"
}

@book{Hun2001,
  title={Applied analysis},
  author={Hunter, John K and Nachtergaele, Bruno},
  year={2001},
  publisher={World Scientific Publishing Company}
}


@techreport{TourismResearch2019,
  title={Tourism Forecasts},
  author={{Tourism Research Australia}},
  year={2019},
  institution={Tourism Research Australia, Canberra}
  }

@article{PanEtAl2020_Geometry,
	title = {Forecast reconciliation: A geometric view with new insights on bias correction},
	journal = {International Journal of Forecasting},
	volume = {37},
	number = {1},
	pages = {343-359},
	year = {2021},
	issn = {0169-2070},
	doi = {https://doi.org/10.1016/j.ijforecast.2020.06.004},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207020300911},
	author = {Anastasios Panagiotelis and George Athanasopoulos and Puwasala Gamakumara and Rob J. Hyndman},
	keywords = {Forecast reconciliation, Projections, Elliptical distributions, Scoring rules, High-dimensional time series},
	abstract = {A geometric interpretation is developed for so-called reconciliation methodologies used to forecast time series that adhere to known linear constraints. In particular, a general framework is established that nests many existing popular reconciliation methods within the class of projections. This interpretation facilitates the derivation of novel theoretical results. First, reconciliation via projection is guaranteed to improve forecast accuracy with respect to a class of loss functions based on a generalised distance metric. Second, the Minimum Trace (MinT) method minimises expected loss for this same class of loss functions. Third, the geometric interpretation provides a new proof that forecast reconciliation using projections results in unbiased forecasts, provided that the initial base forecasts are also unbiased. Approaches for dealing with biased base forecasts are proposed. An extensive empirical study of Australian tourism flows demonstrates the theoretical results of the paper and shows that bias correction prior to reconciliation outperforms alternatives that only bias-correct or only reconcile forecasts.}
}



@Article{ShaHyn2017,
author = {Shang, Han Lin and Hyndman, Rob J.},
title = {Grouped Functional Time Series Forecasting: An Application to Age-Specific Mortality Rates},
journal = {Journal of Computational and Graphical Statistics},
year = {2017},
volume = {26},
number = {2},
pages = {330--343},
issn = {15372715},
archiveprefix = {arXiv},
arxivid = {1609.04222},
doi = {10.1080/10618600.2016.1237877},
eprint = {1609.04222},
isbn = {6126125053},
keywords = {Bottom-up,Forecast reconciliation,Hierarchical time series forecasting,Japanese mortality database,Optimal combination},
publisher = {Taylor {\&} Francis},
}

@article{zarnowitz1987,
title={Consensus and uncertainty in economic prediction},
author={Zarnowitz, Victor and Lambros, Louis A},
journal={Journal of Political Economy},
volume={95},
number={3},
pages={591--621},
year={1987},
publisher={The University of Chicago Press}
}

@techreport{rossi2014,
title={Density forecasts in economics, forecasting and policymaking},
author={Rossi, Barbara},
year={2014},
institution = {Els Opuscles del CREI},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.722.6744}
}

@article{mclean2013,
title={Probabilistic wind vector forecasting using ensembles and {Bayesian} model averaging},
author={McLean Sloughter, J and Gneiting, Tilmann and Raftery, Adrian E},
journal={Monthly Weather Review},
volume={141},
number={6},
pages={2107--2119},
year={2013}
}

@inproceedings{wytock2013,
title={Large-scale probabilistic forecasting in energy systems using sparse {Gaussian} conditional random fields},
author={Wytock, Matt and Kolter, J Zico},
booktitle={2013 IEEE 52nd Annual Conference on Decision and Control (CDC)},
pages={1019--1024},
year={2013},
}

@article{bose2017,
title={Probabilistic demand forecasting at scale},
author={B{\"o}se, Joos-Hendrik and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Lange, Dustin and Salinas, David and Schelter, Sebastian and Seeger, Matthias and Wang, Yuyang},
journal={Proceedings of the VLDB Endowment},
volume={10},
number={12},
pages={1694--1705},
year={2017},
publisher={VLDB Endowment}
}

@article{Taieb2017,
author = {Souhaib {Ben Taieb} and James W. Taylor and Rob J. Hyndman},
title = {Hierarchical Probabilistic Forecasting of Electricity Demand With Smart Meter Data},
journal = {Journal of the American Statistical Association},
year  = {2020},
publisher = {Taylor & Francis},
note = {in press},
doi = {10.1080/01621459.2020.1736081},
}

@incollection{bottou2010,
title={Large-scale machine learning with stochastic gradient descent},
author={Bottou, L{\'e}on},
booktitle={Proceedings of COMPSTAT'2010},
pages={177--186},
editor = {Y Lechevallier and G Saporta},
year={2010},
publisher={Physica-Verlag HD}
}
@article{kingma2013,
  title={Auto-encoding variational {Bayes}},
  author={Kingma, Diederik P and Welling, Max},
  year={2013},
  url = {http://arxiv.org/abs/1312.6114},
}
@article{kingma2014,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  url = {http://arxiv.org/abs/1412.6980},
  year={2014}
}
@article{gneiting2005,
  title={Calibrated probabilistic forecasting using ensemble model output statistics and minimum {CRPS} estimation},
  author={Gneiting, Tilmann and Raftery, Adrian E and Westveld III, Anton H and Goldman, Tom},
  journal={Monthly Weather Review},
  volume={133},
  number={5},
  pages={1098--1118},
  year={2005}
}
@article{carpenter2015,
  title={The {Stan} math library: Reverse-mode automatic differentiation in {C++}},
  author={Carpenter, Bob and Hoffman, Matthew D and Brubaker, Marcus and Lee, Daniel and Li, Peter and Betancourt, Michael},
  year={2015},
  url = {http://arxiv.org/abs/1509.07164},
}

@techreport{szekely2003,
  title={E-Statistics: The energy of statistical samples},
  author={Sz{\'e}kely, G{\'a}bor J},
  Institution={Department of Mathematics and Statistics, Bowling Green State University},
  number={03-05},
  year={2003}
}
@article{dawid2007,
  title={The geometry of proper scoring rules},
  author={Dawid, A Philip},
  journal={Annals of the Institute of Statistical Mathematics},
  volume={59},
  number={1},
  pages={77--93},
  year={2007},
  publisher={Springer}
}

@article{HynKha2008,
author = {Hyndman, R J and Khandakar, Yeasmin},
journal = {Journal of Statistical Software},
number = {3},
pages = {1--22},
title = {Automatic time series forecasting: The forecast package for {R}},
volume = {26},
year = {2008}
}

@Manual{Rfable,
    title = {{fable}: Forecasting Models for Tidy Time Series},
    author = {Mitchell O'Hara-Wild and Rob Hyndman and Earo Wang},
    year = {2020},
    note = {R package version 0.2.0},
    url = {https://CRAN.R-project.org/package=fable},
  }
@article{nystrup2020,
title = "Temporal hierarchies with autocorrelation for load forecasting",
journal = "European Journal of Operational Research",
volume = "280",
number = "3",
pages = "876 - 888",
year = "2020",
issn = "0377-2217",
doi = "https://doi.org/10.1016/j.ejor.2019.07.061",
author = "Peter Nystrup and Erik Lindstrom and Pierre Pinson and Henrik Madsen",
keywords = "Forecasting, Forecast combination, Temporal aggregation, Autocorrelation, Reconciliation",
abstract = "We propose four different estimators that take into account the autocorrelation structure when reconciling forecasts in a temporal hierarchy. Combining forecasts from multiple temporal aggregation levels exploits information differences and mitigates model uncertainty, while reconciliation ensures a unified prediction that supports aligned decisions at different horizons. In previous studies, weights assigned to the forecasts were given by the structure of the hierarchy or the forecast error variances without considering potential autocorrelation in the forecast errors. Our first estimator considers the autocovariance matrix within each aggregation level. Since this can be difficult to estimate, we propose a second estimator that blends autocorrelation and variance information, but only requires estimation of the first-order autocorrelation coefficient at each aggregation level. Our third and fourth estimators facilitate information sharing between aggregation levels using robust estimates of the cross-correlation matrix and its inverse. We compare the proposed estimators in a simulation study and demonstrate their usefulness through an application to short-term electricity load forecasting in four price areas in Sweden. We find that by taking account of auto- and cross-covariances when reconciling forecasts, accuracy can be significantly improved uniformly across all frequencies and areas."
}
@inproceedings{bentaiebkoo,
author = {{Ben Taieb}, Souhaib and Koo, Bonsoo},
year = {2019},
month = {07},
pages = {1337-1347},
title = {Regularized Regression for Hierarchical Forecasting Without Unbiasedness Conditions},
isbn = {978-1-4503-6201-6},
doi = {10.1145/3292500.3330976},
booktitle = {KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}
}
@Manual{snpackage,
    title = {{sn}: The Skew-Normal and Related Distributions such as the Skew-$t$},
    author = {A. Azzalini},
    address = {Universit\`a di Padova, Italia},
    year = {2020},
    note = {R package version 1.6-1},
    url = {https://CRAN.R-project.org/package=sn}
}

@Manual{tsutilspackage,
    title = {{tsutils}: Time Series Exploration, Modelling and Forecasting},
    author = {Nikolaos Kourentzes},
    year = {2019},
    note = {R package version 0.9.0},
    url = {https://CRAN.R-project.org/package=tsutils},
}

@book{HolEtAl2013,
  title={Nonparametric statistical methods},
  author={Hollander, Myles and Wolfe, Douglas A and Chicken, Eric},
  year={2013},
  publisher={John Wiley \& Sons}
}
@article{YaoEtAl2007,
  title={On early stopping in gradient descent learning},
  author={Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  journal={Constructive Approximation},
  volume={26},
  number={2},
  pages={289--315},
  year={2007},
  publisher={Springer}
}
@article{BuhYu2003,
  title={Boosting with the {$L_2$} loss: regression and classification},
  author={B{\"u}hlmann, Peter and Yu, Bin},
  journal={Journal of the American Statistical Association},
  volume={98},
  number={462},
  pages={324--339},
  year={2003},
  publisher={Taylor \& Francis}
}

@Manual{RProbReco,
    title = {{ProbReco}: Score Optimal Probabilistic Forecast Reconciliation},
    author = {Anastasios Panagiotelis},
    year = {2020},
    note = {R package version 0.1.0},
    url = {https://github.com/anastasiospanagiotelis/ProbReco},
  }

